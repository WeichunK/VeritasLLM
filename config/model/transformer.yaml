name: transformer_base
dim: 512
n_layers: 6
n_heads: 8
n_kv_heads: null # If null, defaults to n_heads
vocab_size: 32000
max_seq_len: 1024
norm_eps: 1e-5
rope_theta: 10000.0
dropout: 0.1
attention_backend: flash_attention # options: flash_attention, sdpa, math
activation: swiglu
use_bias: false
moe:
  enabled: false
  num_experts: 8
  num_experts_per_tok: 2
